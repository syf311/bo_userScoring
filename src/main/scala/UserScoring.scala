import org.apache.hadoop.conf.Configuration
import org.apache.spark.SparkContext
import org.apache.spark.SparkContext._
import org.apache.spark.SparkConf
import org.bson.BSONObject
import org.bson.BasicBSONObject

object UserScoring {
  def main(args: Array[String]) {
    //val userFile = args(0)
    //val segFile = args(1)
    //val outputFile = args(2)

    val sc = new SparkContext("local", "UserScoring App")
    
    val userDataConfig = new Configuration()
    val segDataConfig = new Configuration()
    val userScoresConfig = new Configuration()
    userDataConfig.set("mongo.input.uri", "mongodb://127.0.0.1:27017/bo.users2")
    segDataConfig.set("mongo.input.uri", "mongodb://127.0.0.1:27017/bo.segs")
    userDataConfig.set("mongo.job.input.format",
                  "com.mongodb.hadoop.MongoInputFormat");
    segDataConfig.set("mongo.job.input.format",
                  "com.mongodb.hadoop.MongoInputFormat");
    userScoresConfig.set("mongo.output.uri", "mongodb://127.0.0.1:27017/bo.userScores")

    // Input contains tuples of (ObjectId, BSONObject)
    val userData = sc.newAPIHadoopRDD(
        userDataConfig, 
        classOf[com.mongodb.hadoop.MongoInputFormat], 
        classOf[Object], 
        classOf[BSONObject])
        .map{ tuple => 
            val userId = tuple._2.get("userId").toString
            val segIds = tuple._2.get("segIds").asInstanceOf[com.mongodb.BasicDBList]
            val weights = tuple._2.get("weights").asInstanceOf[com.mongodb.BasicDBList]
            val targetAttrs = tuple._2.get("targetAttrs").asInstanceOf[com.mongodb.BasicDBList]
            val attrs = tuple._2.get("selfAttrs").asInstanceOf[com.mongodb.BasicDBList]
            (userId, segIds, weights, targetAttrs, attrs)
        }
        .cache()

    /*val userData = sc.textFile(userFile)
    	.filter(line => !line.startsWith("#"))
    	.map{ line => 
    		val columns = line.split(" ")
    		val userId = columns(0)
    		val segIds = columns(1).substring(1, columns(1).length() - 1).split(",")
    		val weights = columns(2).substring(1, columns(2).length() - 1).split(",")
			val targetAttrs = columns(3).substring(1, columns(3).length() - 1).split(",")
			val attrs = columns(4).substring(1, columns(4).length() - 1).split(",")
    		(userId, segIds, weights, targetAttrs, attrs)
    	}
    	.cache()*/
    val segData = sc.newAPIHadoopRDD(
        segDataConfig, 
        classOf[com.mongodb.hadoop.MongoInputFormat], 
        classOf[Object], 
        classOf[BSONObject])
        .map{ tuple => 
            val segId = tuple._2.get("segId").toString
            segId
        }
        .cache()
    //val segData= sc.textFile(segFile).filter(line => !line.startsWith("#")).cache()

    val userProdSeg = userData
    	.map{ case (userId, segIds, weights, targetAttrs, attrs) => 
    		(userId, weights, targetAttrs)
    	}.cartesian(segData)
    
    val targetUserData = userData
    	.map{ case (targetUserId, segIds, weights, targetAttrs, attrs) => 
    		(targetUserId, segIds, attrs)
    	}
    
    val userScores = userProdSeg.cartesian(targetUserData)
    	.filter{ userSegTargetUser => 
    		val userId = userSegTargetUser._1._1._1 
    		val segId = userSegTargetUser._1._2
    		val targetUserId = userSegTargetUser._2._1
    		val targetUserSegIds = userSegTargetUser._2._2
    		userId != targetUserId && targetUserSegIds.contains(segId)
    	}
    	.map{
    		userSegTargetUser =>
    		val userId = userSegTargetUser._1._1._1 
    		val segId = userSegTargetUser._1._2
    		val weights = userSegTargetUser._1._1._2
    		val targetAttrs = userSegTargetUser._1._1._3

    		val targetUserId = userSegTargetUser._2._1
    		val targetUserAttrs = userSegTargetUser._2._3

    		var score = 0.0;
    		var i = 0;
    		while (i < targetAttrs.size()) {
    			if (targetAttrs.get(i) == targetUserAttrs.get(i)) {
    				score = weights.get(i).asInstanceOf[Double] + score
    			}

    			i += 1
    		}

    		((userId, segId), targetUserId, score)
    	}
    	//.sortBy(_._3, false)
        .map((tuple) => {
            var bson = new BasicBSONObject()
            bson.put("userId", tuple._1._1)
            bson.put("segId", tuple._1._2)
            bson.put("targetUserId", tuple._2)
            bson.put("score", tuple._3.asInstanceOf[java.lang.Double])
            // Output contains tuples of (null, BSONObject) - ObjectId will be generated by Mongo driver if null
            (null, bson)
        })


    //userScores.saveAsTextFile(outputFile)
    // The path argument is unused; all documents will go to 'mongo.output.uri'.
    userScores.saveAsNewAPIHadoopFile(
        "file:///this-is-completely-unused", 
        classOf[Any], 
        classOf[Any], 
        classOf[com.mongodb.hadoop.MongoOutputFormat[Any, Any]], 
        userScoresConfig)
  }
}
